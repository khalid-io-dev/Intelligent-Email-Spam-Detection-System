{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef357d24",
   "metadata": {},
   "source": [
    "# Systeme Intelligent de Detection de Spams - BMSecurity\n",
    "## Analyse des Données d'Emails pour Classification NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9474af46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation de SparkSession...\n",
      "✓ Spark version: 4.0.1\n",
      "✓ Nombre de cœurs utilisés: 22\n",
      "✓ Mémoire driver: 8GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://127.0.0.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BankAttritionPrediction</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x22e22388310>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"org\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"akka\").setLevel(logging.ERROR)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"Initialisation de SparkSession...\")\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"BankAttritionPrediction\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"16\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"✓ Spark version: {spark.version}\")\n",
    "print(f\"✓ Nombre de cœurs utilisés: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"✓ Mémoire driver: 8GB\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e57b96e",
   "metadata": {},
   "source": [
    "## Section 1 : Importation des Librairies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540d592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, when, isnan, isnull, lower, regexp_replace, split, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c94da0",
   "metadata": {},
   "source": [
    "## Section 2 : Chargement et Exploration du Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7018d354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STRUCTURE DU DATASET ===\n",
      "Dimensions: 31716 lignes, 8 colonnes\n",
      "\n",
      "Types de données:\n",
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- message_id: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      " |-- label_text: string (nullable = true)\n",
      " |-- subject: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      "\n",
      "\n",
      "Premières lignes:\n",
      "+-------+-----------------+------------------+--------------------+------------------+----------+--------------------+--------------------+\n",
      "|summary|              _c0|        message_id|                text|             label|label_text|             subject|             message|\n",
      "+-------+-----------------+------------------+--------------------+------------------+----------+--------------------+--------------------+\n",
      "|  count|            31716|             31716|               31665|             31716|     31716|               31442|               31371|\n",
      "|   mean|          15857.5|16854.187539412284|                NULL|0.5096165973010468|      NULL|            386429.0|            71403.25|\n",
      "| stddev|9155.764905238666| 9734.616391716854|                NULL|0.4999153936875302|      NULL|  481877.22667086066|   136659.2035841348|\n",
      "|    min|                0|                 0|\u001b ( b \u001b $ b ! zck...|                 0|       ham|               \u001b ( b|\u0001 & who wants to ...|\n",
      "|    max|            31715|             33715|þquieres felicita...|                 1|      spam|þquieres felicita...|þya planeó dónde ...|\n",
      "+-------+-----------------+------------------+--------------------+------------------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('DataSet_Emails.csv', header=True, inferSchema=True,multiLine=True , escape='\"')\n",
    "\n",
    "print(\"=== STRUCTURE DU DATASET ===\")\n",
    "print(f\"Dimensions: {df.count()} lignes, {len(df.columns)} colonnes\")\n",
    "print(f\"\\nTypes de données:\")\n",
    "df.printSchema()\n",
    "print(f\"\\nPremières lignes:\")\n",
    "# print(f\"\\nInformations générales:\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adcd207b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+-----+----------+--------------------+--------------------+----------+\n",
      "|_c0|message_id|                text|label|label_text|             subject|             message|      date|\n",
      "+---+----------+--------------------+-----+----------+--------------------+--------------------+----------+\n",
      "|  0|     33214|any software just...|    1|      spam|any software just...|understanding oem...|2005-06-18|\n",
      "|  1|     11929|perspective on fe...|    0|       ham|perspective on fe...|19 th , 2 : 00 pm...|2001-06-19|\n",
      "|  2|     19784|wanted to try ci ...|    1|      spam|wanted to try ci ...|viagra at $ 1 . 1...|2004-09-11|\n",
      "|  3|      2209|enron / hpl actua...|    0|       ham|enron / hpl actua...|teco tap 30 . 000...|2000-12-12|\n",
      "|  4|     15880|looking for cheap...|    1|      spam|looking for cheap...|water past also ,...|2005-02-13|\n",
      "|  5|     15726|emerging growth s...|    1|      spam|emerging growth s...|vera ,\\nvcsc - br...|2005-01-18|\n",
      "|  6|     21384|internet provider...|    1|      spam|internet provider...|i noticed that yo...|2005-02-24|\n",
      "|  7|      9556|[ avfs ] romanian...|    1|      spam|[ avfs ] romanian...|to : avfs @ fazek...|2002-04-21|\n",
      "|  8|      5458|fortune most admi...|    0|       ham|fortune most admi...|congratulations !...|2000-02-07|\n",
      "|  9|     11027|localized softwar...|    1|      spam|localized softwar...|hello , we would ...|2005-07-22|\n",
      "| 10|     15070|please your ma te...|    1|      spam|   please your ma te|hey fox ,\\ni have...|2004-08-11|\n",
      "| 11|     18127|merlin cry constr...|    1|      spam|merlin cry constr...|hi ,\\ngenierc and...|2004-01-04|\n",
      "| 12|     22721|re : risk positio...|    0|       ham|re : risk positio...|? ? ? ? thanks\\n-...|2000-03-29|\n",
      "| 13|     14239|enron mentions en...|    0|       ham|      enron mentions|enron seeks money...|2001-12-03|\n",
      "| 14|      2770|re : boat i belie...|    0|       ham|           re : boat|i believe the boa...|2001-03-20|\n",
      "| 15|     11827|agenda for prc co...|    0|       ham|agenda for prc co...|all :\\nattached p...|2001-05-29|\n",
      "| 16|     24916|get on the bus . ...|    1|      spam|get on the bus . ...|click here to see...|2002-09-17|\n",
      "| 17|     30575|re [ 11 ] bands l...|    1|      spam|           re [ 11 ]|bands leonardo di...|2004-11-26|\n",
      "| 18|      3123|re : big cowboy /...|    0|       ham|re : big cowboy /...|please see the at...|2001-04-25|\n",
      "| 19|      3196|tenaska darren ,\\...|    0|       ham|             tenaska|darren ,\\nattache...|2001-05-07|\n",
      "+---+----------+--------------------+-----+----------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef66d5d7",
   "metadata": {},
   "source": [
    "## Section 3 : Évaluation de la Qualité des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1f27aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALEURS MANQUANTES ===\n",
      "+---+----------+----+-----+----------+-------+-------+----+\n",
      "|_c0|message_id|text|label|label_text|subject|message|date|\n",
      "+---+----------+----+-----+----------+-------+-------+----+\n",
      "|  0|         0|  51|    0|         0|    274|    345|   0|\n",
      "+---+----------+----+-----+----------+-------+-------+----+\n",
      "\n",
      "\n",
      "Pourcentage de valeurs manquantes:\n",
      "+---+----------+-------------------+-----+----------+------------------+------------------+----+\n",
      "|_c0|message_id|               text|label|label_text|           subject|           message|date|\n",
      "+---+----------+-------------------+-----+----------+------------------+------------------+----+\n",
      "|0.0|       0.0|0.16080211880438897|  0.0|       0.0|0.8639172657333837|1.0877790389708664| 0.0|\n",
      "+---+----------+-------------------+-----+----------+------------------+------------------+----+\n",
      "\n",
      "\n",
      "=== DOUBLONS ===\n",
      "Nombre de doublons: 0\n",
      "Pourcentage de doublons: 0.00%\n",
      "Dataset après suppression des valeurs manquantes: 31148 lignes restantes\n",
      "\n",
      "Dataset final: 31148 lignes, 8 colonnes\n"
     ]
    }
   ],
   "source": [
    "print(\"=== VALEURS MANQUANTES ===\")\n",
    "# Count missing values for each column\n",
    "missing_counts = df.select([count(when(col(c).isNull(), 1)).alias(c) for c in df.columns])\n",
    "missing_counts.show()\n",
    "\n",
    "total_rows = df.count()\n",
    "print(f\"\\nPourcentage de valeurs manquantes:\")\n",
    "missing_pct = df.select([\n",
    "    (count(when(col(c).isNull(), 1)) / total_rows * 100).alias(c) \n",
    "    for c in df.columns\n",
    "])\n",
    "missing_pct.show()\n",
    "\n",
    "print(\"\\n=== DOUBLONS ===\")\n",
    "duplicates_count = df.count() - df.dropDuplicates().count()\n",
    "print(f\"Nombre de doublons: {duplicates_count}\")\n",
    "print(f\"Pourcentage de doublons: {(duplicates_count / total_rows * 100):.2f}%\")\n",
    "\n",
    "df_clean = df.dropDuplicates()\n",
    "if duplicates_count > 0:\n",
    "    df = df_clean\n",
    "    print(f\"Dataset nettoyé: {df.count()} lignes restantes\")\n",
    "\n",
    "# Drop rows with null values\n",
    "df = df.dropna()\n",
    "final_rows = df.count()\n",
    "print(f\"Dataset après suppression des valeurs manquantes: {final_rows} lignes restantes\")\n",
    "\n",
    "print(f\"\\nDataset final: {final_rows} lignes, {len(df.columns)} colonnes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987c581",
   "metadata": {},
   "source": [
    "## Section 4 : Analyse de la Distribution de la Variable Cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df474ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert Spark dataframe to Pandas for easier analysis\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# Distribution of target variable\n",
    "print(\"=== DISTRIBUTION DE LA VARIABLE CIBLE ===\")\n",
    "print(pdf['label'].value_counts())\n",
    "print(\"\\nPourcentage de distribution:\")\n",
    "print(pdf['label'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar plot\n",
    "pdf['label'].value_counts().plot(kind='bar', ax=axes[0], color=['green', 'red'])\n",
    "axes[0].set_title('Distribution Spam vs Ham')\n",
    "axes[0].set_xlabel('Type')\n",
    "axes[0].set_ylabel('Nombre')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "pdf['label'].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%', colors=['green', 'red'])\n",
    "axes[1].set_title('Proportion Spam vs Ham')\n",
    "axes[1].set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDataset équilibré: {'Oui' if abs(pdf['label'].value_counts()[0] - pdf['label'].value_counts()[1]) / len(pdf) < 0.1 else 'Non - Déséquilibre détecté'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c86ec7",
   "metadata": {},
   "source": [
    "## Section 5 : WordClouds - Analyse des Mots Fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wordcloud -q\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Séparer les emails spam et ham\n",
    "spam_text = ' '.join(pdf[pdf['label'] == 'spam']['text'].astype(str))\n",
    "ham_text = ' '.join(pdf[pdf['label'] == 'ham']['text'].astype(str))\n",
    "\n",
    "# WordCloud pour les emails Spam\n",
    "print(\"Génération WordCloud - SPAM\")\n",
    "wc_spam = WordCloud(width=1000, height=500, background_color='white', colormap='Reds', max_words=100).generate(spam_text)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(wc_spam, interpolation='bilinear')\n",
    "plt.title('Mots les Plus Fréquents - EMAILS SPAM', fontsize=16, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# WordCloud pour les emails Ham\n",
    "print(\"Génération WordCloud - HAM\")\n",
    "wc_ham = WordCloud(width=1000, height=500, background_color='white', colormap='Greens', max_words=100).generate(ham_text)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(wc_ham, interpolation='bilinear')\n",
    "plt.title('Mots les Plus Fréquents - EMAILS LEGIT (HAM)', fontsize=16, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c78b386",
   "metadata": {},
   "source": [
    "## Section 6 : Prétraitement du Texte - Pipeline NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0fc6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # 1. Normalisation: convertir en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Supprimer ponctuation et caractères spéciaux\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenisation\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Supprimer stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # 5. Stemming avec PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Pipeline NLP - Étapes:\")\n",
    "print(\"✓ Normalisation (minuscules)\")\n",
    "print(\"✓ Suppression ponctuation & caractères spéciaux\")\n",
    "print(\"✓ Tokenisation\")\n",
    "print(\"✓ Suppression stopwords\")\n",
    "print(\"✓ Stemming (PorterStemmer)\")\n",
    "\n",
    "# Application du preprocessing\n",
    "print(\"\\nApplication du preprocessing...\")\n",
    "pdf['processed_text'] = pdf['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"\\nExemple de preprocessing:\")\n",
    "print(f\"Avant: {pdf.iloc[0]['text'][:100]}...\")\n",
    "print(f\"Après: {pdf.iloc[0]['processed_text'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37675eae",
   "metadata": {},
   "source": [
    "## Section 7 : Vectorisation du Texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a078646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Préparation des données\n",
    "X = pdf['processed_text']\n",
    "y = pdf['label']\n",
    "\n",
    "# Split train/test (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Split des données:\")\n",
    "print(f\"Training: {len(X_train)} samples\")\n",
    "print(f\"Testing: {len(X_test)} samples\")\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "print(\"\\n7.1 Vectorisation TF-IDF\")\n",
    "tfidf_vec = TfidfVectorizer(max_features=3000, min_df=2, max_df=0.8)\n",
    "X_train_tfidf = tfidf_vec.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vec.transform(X_test)\n",
    "\n",
    "print(f\"Dimension TF-IDF: {X_train_tfidf.shape}\")\n",
    "print(f\"Nombre de features extraites: {len(tfidf_vec.get_feature_names_out())}\")\n",
    "\n",
    "# Count Vectorization\n",
    "print(\"\\n7.2 Vectorisation Count\")\n",
    "count_vec = CountVectorizer(max_features=3000, min_df=2, max_df=0.8)\n",
    "X_train_count = count_vec.fit_transform(X_train)\n",
    "X_test_count = count_vec.transform(X_test)\n",
    "\n",
    "print(f\"Dimension Count: {X_train_count.shape}\")\n",
    "print(f\"Nombre de features extraites: {len(count_vec.get_feature_names_out())}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f579374",
   "metadata": {},
   "source": [
    "## Section 8 : Entraînement des Modèles ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177cb42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# ===== MODÈLE 1: NAIVE BAYES =====\n",
    "print(\"=\"*70)\n",
    "print(\"8.1 MODELE 1 - NAIVE BAYES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "precision_nb = precision_score(y_test, y_pred_nb, average='weighted')\n",
    "recall_nb = recall_score(y_test, y_pred_nb, average='weighted')\n",
    "f1_nb = f1_score(y_test, y_pred_nb, average='weighted')\n",
    "\n",
    "print(f\"Accuracy:  {accuracy_nb:.4f} ({accuracy_nb*100:.2f}%)\")\n",
    "print(f\"Precision: {precision_nb:.4f}\")\n",
    "print(f\"Recall:    {recall_nb:.4f}\")\n",
    "print(f\"F1-Score:  {f1_nb:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "# Confusion Matrix for Naive Bayes\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', xticklabels=['ham', 'spam'], yticklabels=['ham', 'spam'])\n",
    "plt.title('Confusion Matrix - Naive Bayes')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== MODÈLE 2: LOGISTIC REGRESSION =====\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"8.2 MODELE 2 - LOGISTIC REGRESSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "precision_lr = precision_score(y_test, y_pred_lr, average='weighted')\n",
    "recall_lr = recall_score(y_test, y_pred_lr, average='weighted')\n",
    "f1_lr = f1_score(y_test, y_pred_lr, average='weighted')\n",
    "\n",
    "print(f\"Accuracy:  {accuracy_lr:.4f} ({accuracy_lr*100:.2f}%)\")\n",
    "print(f\"Precision: {precision_lr:.4f}\")\n",
    "print(f\"Recall:    {recall_lr:.4f}\")\n",
    "print(f\"F1-Score:  {f1_lr:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "# Confusion Matrix for Logistic Regression\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Greens', xticklabels=['ham', 'spam'], yticklabels=['ham', 'spam'])\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== MODÈLE 3: SVM (Linear) =====\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"8.3 MODELE 3 - SVM (LINEAR)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "svm_model = LinearSVC(max_iter=2000, random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "precision_svm = precision_score(y_test, y_pred_svm, average='weighted')\n",
    "recall_svm = recall_score(y_test, y_pred_svm, average='weighted')\n",
    "f1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n",
    "\n",
    "print(f\"Accuracy:  {accuracy_svm:.4f} ({accuracy_svm*100:.2f}%)\")\n",
    "print(f\"Precision: {precision_svm:.4f}\")\n",
    "print(f\"Recall:    {recall_svm:.4f}\")\n",
    "print(f\"F1-Score:  {f1_svm:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "# Confusion Matrix for SVM\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Reds', xticklabels=['ham', 'spam'], yticklabels=['ham', 'spam'])\n",
    "plt.title('Confusion Matrix - SVM')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e80048",
   "metadata": {},
   "source": [
    "## Section 9 : Comparaison des Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db8a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Créer un dataframe de comparaison\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Naive Bayes', 'Logistic Regression', 'SVM (Linear)'],\n",
    "    'Accuracy': [accuracy_nb, accuracy_lr, accuracy_svm],\n",
    "    'Precision': [precision_nb, precision_lr, precision_svm],\n",
    "    'Recall': [recall_nb, recall_lr, recall_svm],\n",
    "    'F1-Score': [f1_nb, f1_lr, f1_svm]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPARAISON GLOBALE DES MODELES\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.bar(comparison_df['Model'], comparison_df[metric], color=colors)\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_ylim([0, 1.1])\n",
    "    ax.tick_params(axis='x', rotation=15)\n",
    "    for i, v in enumerate(comparison_df[metric]):\n",
    "        ax.text(i, v + 0.02, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Meilleur modèle\n",
    "best_model_idx = comparison_df['F1-Score'].idxmax()\n",
    "best_model = comparison_df.iloc[best_model_idx]\n",
    "print(f\"\\n✓ Meilleur modèle: {best_model['Model']} (F1-Score: {best_model['F1-Score']:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a26bbe5",
   "metadata": {},
   "source": [
    "## Section 10 : Export et Sauvegarde du Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0997cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Créer un dossier pour les modèles\n",
    "model_dir = 'ml_models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Sauvegarder les meilleurs modèles et vectoriseurs\n",
    "print(\"Sauvegarde des modèles...\\n\")\n",
    "\n",
    "# Choisir le meilleur modèle\n",
    "models = {\n",
    "    'naive_bayes': (nb_model, 'Naive Bayes'),\n",
    "    'logistic_regression': (lr_model, 'Logistic Regression'),\n",
    "    'svm': (svm_model, 'SVM')\n",
    "}\n",
    "\n",
    "# Sauvegarder tous les modèles et le vectoriseur\n",
    "for name, (model, display_name) in models.items():\n",
    "    model_path = os.path.join(model_dir, f'{name}_model.joblib')\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"✓ Saved: {model_path}\")\n",
    "\n",
    "# Sauvegarder le vectoriseur TF-IDF\n",
    "tfidf_path = os.path.join(model_dir, 'tfidf_vectorizer.joblib')\n",
    "joblib.dump(tfidf_vec, tfidf_path)\n",
    "print(f\"✓ Saved: {tfidf_path}\")\n",
    "\n",
    "# Sauvegarder le dataset préprocessé\n",
    "preprocessed_path = 'preprocessed_dataset.csv'\n",
    "pdf.to_csv(preprocessed_path, index=False)\n",
    "print(f\"✓ Saved: {preprocessed_path}\")\n",
    "\n",
    "# Sauvegarder les résultats d'évaluation\n",
    "results_path = os.path.join(model_dir, 'model_results.csv')\n",
    "comparison_df.to_csv(results_path, index=False)\n",
    "print(f\"✓ Saved: {results_path}\")\n",
    "\n",
    "print(\"\\n✓ Tous les modèles ont été sauvegardés avec succès!\")\n",
    "print(\"\\nStructure des fichiers créés:\")\n",
    "print(f\"  {model_dir}/\")\n",
    "print(f\"    ├── naive_bayes_model.joblib\")\n",
    "print(f\"    ├── logistic_regression_model.joblib\")\n",
    "print(f\"    ├── svm_model.joblib\")\n",
    "print(f\"    ├── tfidf_vectorizer.joblib\")\n",
    "print(f\"    └── model_results.csv\")\n",
    "print(f\"  {preprocessed_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
